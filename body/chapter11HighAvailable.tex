\part{集群方案篇}

本章介绍几种高可用的解决方案。

\chapter{集群基础知识}

\section{集群概述}

集群是一组协同工作的服务实体，用以提供比单一服务实体更具扩展性和可用性
的服务平台。

在客户端看来，一个集群就是一个完整不可细分的实体，但事实上一个集群实体
是由完成不同任务的服务节点个体所组成的。

集群实体的可扩展性是指，在集群运行中新的服务节点可以动态的加入集群实体
从而提升集群实体的综合性能。

集群实体的高可用性是指，集群实体通过其内部的服务节点的冗余使客户端免于
OUT OF SERVICE错误。简单的说，在集群中同一服务可以由多个服务节点提供，
当部分服务节点失效后，其他服务节点可以接管服务。

集群实体地址是指客户端访问集群实体获得服务资源的唯一入口地址。

负载均衡是指集群中的分发设备（服务）将用户的请求任务比较均衡（不是平均）
分发到集群实体中的服务节点计算、存储和网络资源中。一般我们将提供负载均
衡分发的设备叫做负载均衡器。负载均衡器一般具备如下三个功能：

\begin{enumerate}[itemsep=0pt,parsep=0pt]
\item 维护集群地址
\item 负责管理各个服务节点的加入和退出
\item 集群地址向内部服务节点地址的转换
\end{enumerate}

错误恢复是指集群中某个节点或某些服务节点（设备）不能正常工作（或提供服
  务），其他类似服务节点（设备）可以资源透明和持续的完成原有任务。具备
错误恢复能力是集群实体高可用性的必要条件。

负载均衡和错误恢复都需要集群实体中各个服务节点中有执行同一任务的资源存
在，而且对于同一任务的各个资源来说，执行任务所学的信息视图必须一致。

\section{集群类型}

多台同构或异构的计算机用某种方式连接起来协同完成特定的任务就构成了集群
系统，目前Linux下的集群主要有三种类型：

\begin{enumerate}[itemsep=0pt,parsep=0pt]
\item HA（High Availability）
\item LB（Load Balancing）
\item HPC（High Performance Computing）
\end{enumerate}

\input{body/chapter11_keepalived}

\input{body/chapter11_lvs_keepalived}

\input{body/chapter11_heartbeat}

\input{body/chapter11_nginx_keepalived}

\chapter{Pacemaker+Corosync高可用集群}

高可用集群，是指以减少服务中断时间为目的的服务器集群技术。高可用集群的
出现是为了减少由计算机硬件和软件易错性所带来的损失。它通过保护用户的业
务程序对外不间断提供的服务，把因软件/硬件/人为造成的故障对业务的影响降
低到最小程度。如果某个节点失效，它的备援节点将在数秒钟的时间内接管它的
职责。因此，对于用户而言，集群永远不会停机。高可用集群软件的主要作用就
是实现故障检查和业务切换的自动化。

下面案例是httpd服务的具体实现：

\section{准备工作}

本案例的拓扑图为：

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.5\textwidth]{img/web_topo.png}
  \caption{corosync实验拓扑}
\end{figure}

为了配置一台主机成为HA的节点，通常需要做出如下的准备工作：
\begin{enumerate}[itemsep=0pt,parsep=0pt]
  \item 所有节点的主机名和对应的IP地址解析服务可以正常工作，且每个节点
    的主机名称需要跟“uname -n”命令的结果保持一致；因此，需要保证两个节
    点上的/etc/hosts文件一致；
  \item 两个节点可以基于密钥进行ssh通信；
\end{enumerate}

具体实现为：

\begin{verbatim}
[root@master ~]# cat /etc/hosts
192.168.56.101   master.liucc.com master
192.168.56.102	 slave.liucc.com slave
\end{verbatim}

为了使得重新启动系统后仍能保持如上的主机名称，还需要分别在各节点修改
/etc/sysconfig/network文件。

\begin{verbatim}
master:
# sed -i 's@\(HOSTNAME=\).*@\1master.liucc.com@g'  /etc/sysconfig/network
# hostname master.liucc.com

slave:
# sed -i 's@\(HOSTNAME=\).*@\1slave.liucc.com@g' /etc/sysconfig/network
# hostname slave.liucc.com
\end{verbatim}

各节点生成ssh密钥：

\begin{verbatim}
master:
# cd /root/.ssh
# ssh-keygen -t rsa
# scp id_rsa.pub slave:/root/.ssh/authorized_keys

slave:
# cd /root/.ssh
# ssh-keygen -t rsa
# cat id_rsa.pub >> authorized_keys
# scp authorized_keys master:/root/.ssh
\end{verbatim}

\section{安装软件包}

首先确保已配置本地yum源，然后安装如下依赖包：

\begin{verbatim}
[root@master ~] # yum -y install libibverbs librdmacm lm_sensors \
> libtool-ltdl openhpi-libs openhpi perl-TimeDate
\end{verbatim}

安装完毕依赖包，接下来安装corosync及pacemaker，这里已经准备了相应的软件包，

\begin{verbatim}
[root@master ~]# ls clusterlab
cluster-glue-1.0.6-1.6.el5.i386.rpm       
cluster-glue-libs-1.0.6-1.6.el5.i386.rpm  
corosync-1.2.7-1.1.el5.i386.rpm           
corosynclib-1.2.7-1.1.el5.i386.rpm        
heartbeat-3.0.3-2.3.el5.i386.rpm          
heartbeat-libs-3.0.3-2.3.el5.i386.rpm
libesmtp-1.0.4-5.el5.i386.rpm
pacemaker-1.1.5-1.1.el5.i386.rpm
pacemaker-libs-1.1.5-1.1.el5.i386.rpm
resource-agents-1.0.4-1.1.el5.i386.rpm

[root@master ~]# cd clusterlab
[root@master clusterlab]# yum -y --nogpgcheck localinstall *.rpm
Total size: 17 M
Downloading Packages:
Running rpm_check_debug
Running Transaction Test
Finished Transaction Test
Transaction Test Succeeded
Running Transaction
  Installing     : libesmtp
  Installing     : cluster-glue-libs
  Installing     : corosynclib
  Installing     : cluster-glue
  Installing     : resource-agents 
  Installing     : corosync
  Installing     : heartbeat-libs
  Installing     : pacemaker
  Installing     : pacemaker-libs 
  Installing     : heartbeat
Installed products updated.

Complete!
\end{verbatim}

\section{配置及启动corosync}

下面的配置在master上完成，编辑/etc/corosync/corosync.conf

\begin{verbatim}
[root@master ~]# cd /etc/corosync
[root@master corosync]# cp corosync.conf.example corosync.conf
\end{verbatim}

在文件尾部添加如下内容：

\begin{verbatim}
[root@master corosync]# tail corosync.conf
service {
  ver:  0
  name: pacemaker
}

aisexec {
  user: root
  group:  root
}
\end{verbatim}

并设定此配置文件中 bindnetaddr后面的IP地址为你的网卡所在网络的网络地址，
我们这里的两个节点在192.168.56.0网络，因此这里将其设定为192.168.56.0。
整个配置文件如下：

\begin{verbatim}
[root@master corosync]# cat corosync.conf
# Please read the corosync.conf.5 manual page
compatibility: whitetank

totem {
        version: 2 #版本号
        secauth: off #安全认证
        threads: 0 #线程数，根据CPU个数和核心数确定
        interface {
	        ringnumber: 0 #冗余环号，节点有多个网卡是可定义对应网卡在一个环内
	        bindnetaddr: 192.168.56.0 # 绑定网络地址
	        mcastaddr: 226.94.1.1     # 心跳信息所使用的组播地址
	        mcastport: 5405           # 心跳组播使用的端口
        }
}

logging {
        fileline: off #指定要打印的行
        to_stderr: no #是否发送到标准错误输出
        to_logfile: yes #记录到文件
        to_syslog: yes #记录到syslog
        logfile: /var/log/cluster/corosync.log # corosync日志文件
        debug: off
        timestamp: on #是否打印时间戳，利于定位错误（会消耗CPU资源）
        logger_subsys {
	        subsys: AMF
	        debug: off
        }
}

amf {
        mode: disabled
}

service {
        ver: 0
        name: pacemaker # 定义corosync在启动时自动启动pacemaker
}

aisexec {               # 表示启动corosync的ais功能，以哪个用户身份运行
        user: root
        group: root
}
\end{verbatim}

生成节点间通信时用到的认证密钥文件：

\begin{verbatim}
[root@master corosync]# corosync-keygen
[root@master corosync]# scp -p corosync.conf authkey slave:/etc/corosync

建立corosync日志所需要的目录
[root@master corosync]# mkdir /var/log/cluster
[root@master corosync]# ssh slave "mkdir /var/log/cluster"
\end{verbatim}

接下来就可以启动corosync了，

\begin{verbatim}
[root@master ~]# /etc/init.d/corosync start
Starting Corosync Cluster Engine (corosync):               [  OK  ]
\end{verbatim}

此时，可以查看一下关于corosync引擎是否正常启动的系统日志，

\begin{verbatim}
[root@master corosync]# grep -e "Corosync Cluster Engine" -e "configuration file" /var/log/messages
Feb 28 11:17:02 unionpay smartd[2787]: Opened configuration file /etc/smartd.conf 
Feb 28 11:21:24 master smartd[2770]: Opened configuration file /etc/smartd.conf 
Feb 28 11:44:06 master smartd[2766]: Opened configuration file /etc/smartd.conf 
Feb 28 13:43:05 master corosync[7859]:   [MAIN  ] Corosync Cluster Engine ('1.2.7'): started and ready to provide service.
Feb 28 13:43:05 master corosync[7859]:   [MAIN  ] Successfully read main configuration file '/etc/corosync/corosync.conf'.
\end{verbatim}

查看初始化成员节点通知是否正常发出，
\begin{verbatim}
[root@master corosync]# grep TOTEM /var/log/messages
Feb 28 13:43:05 master corosync[7859]:   [TOTEM ] Initializing transport (UDP/IP).
Feb 28 13:43:05 master corosync[7859]:   [TOTEM ] Initializing transmit/receive security: libtomcrypt SOBER128/SHA1HMAC (mode 0).
Feb 28 13:43:05 master corosync[7859]:   [TOTEM ] The network interface [192.168.56.101] is now up.
Feb 28 13:43:05 master corosync[7859]:   [TOTEM ] A processor joined or left the membership and a new membership was formed.
\end{verbatim}

检查启动过程中是否有错误产生：
\begin{verbatim}
# grep ERROR: /var/log/messages | grep -v unpack_resources
\end{verbatim}

查看pacemaker是否正常启动：
\begin{verbatim}
[root@master corosync]#  grep pcmk_startup /var/log/messages
Feb 28 13:43:05 master corosync[7859]:   [pcmk  ] info: pcmk_startup: CRM: Initialized
Feb 28 13:43:05 master corosync[7859]:   [pcmk  ] Logging: Initialized pcmk_startup
Feb 28 13:43:05 master corosync[7859]:   [pcmk  ] info: pcmk_startup: Maximum core file size is: 4294967295
Feb 28 13:43:05 master corosync[7859]:   [pcmk  ] info: pcmk_startup: Service: 9
Feb 28 13:43:05 master corosync[7859]:   [pcmk  ] info: pcmk_startup: Local hostname: master.liucc.com
\end{verbatim}

如果上面命令执行均没有问题，接着可以执行如下命令启动node2上的corosync
\begin{verbatim}
[root@slave cluster]# ssh slave -- /etc/init.d/corosync start
The authenticity of host 'slave (192.168.56.102)' can't be established.
RSA key fingerprint is 3e:36:02:7a:4c:3c:0d:39:d0:51:a4:86:0f:85:97:c8.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'slave' (RSA) to the list of known hosts.
Starting Corosync Cluster Engine (corosync): [  OK  ]
[root@slave cluster]# ssh slave -- /etc/init.d/corosync status
corosync (pid  10867) is running...
\end{verbatim}

注意：启动node2需要在node1上使用如上命令进行，不要在node2节点上直接启动；

使用如下命令查看集群节点的启动状态：
\begin{verbatim}
[root@master corosync]# crm status
============
Last updated: Sat Feb 28 13:48:11 2015
Stack: openais
Current DC: master.liucc.com - partition with quorum
Version: 1.1.5-1.1.el5-01e86afaaa6d4a8c4836f68df80ababd6ca3902f
2 Nodes configured, 2 expected votes
0 Resources configured.
============

Online: [ master.liucc.com slave.liucc.com ]

\end{verbatim}

从上面的信息可以看出两个节点都已经正常启动，并且集群已经处于正常工作状
态。

由于corosync默认启用了stonith，而当前集群并没有相应的stonith设备，因此
此默认配置目前尚不可用，这可以通过如下命令验正：

\begin{verbatim}
[root@master corosync]# crm_verify -L
crm_verify[7926]: 2015/02/28_13:51:01 ERROR: unpack_resources: Resource start-up disabled since no STONITH resources have been defined
crm_verify[7926]: 2015/02/28_13:51:01 ERROR: unpack_resources: Either configure some or disable STONITH with the stonith-enabled option
crm_verify[7926]: 2015/02/28_13:51:01 ERROR: unpack_resources: NOTE: Clusters with shared data need STONITH to ensure data integrity
Errors found during check: config not valid
  -V may provide more details

\end{verbatim}

我们里可以通过如下命令先禁用stonith：

\begin{verbatim}
[root@master corosync]# crm configure property stonith-enabled=false
\end{verbatim}

使用如下命令查看当前的配置信息：

\begin{verbatim}
[root@master corosync]# crm configure show
node master.liucc.com
node slave.liucc.com
property $id="cib-bootstrap-options" \
	dc-version="1.1.5-1.1.el5-01e86afaaa6d4a8c4836f68df80ababd6ca3902f" \
	cluster-infrastructure="openais" \
	expected-quorum-votes="2" \
	stonith-enabled="false"
\end{verbatim}

从中可以看出stonith已经被禁用。上面的crm，crm\_verify命令是1.0后的版本的
pacemaker提供的基于命令行的集群管理工具；可以在集群中的任何一个节点上执
行。

\section{为集群添加资源}

corosync支持heartbeat，LSB和ocf等类型的资源代理，目前较为常用的类型为
LSB和OCF两类，stonith类专为配置stonith设备而用；可以通过如下命令查看当
前集群系统所支持的类型：

\begin{verbatim}
# crm ra classes 
heartbeat
lsb
ocf / heartbeat pacemaker
stonith
\end{verbatim}

如果想要查看某种类别下的所用资源代理的列表，可以使用类似如下命令实现：

\begin{verbatim}
[root@master ~]# crm ra list lsb
NetworkManager      acpid               anacron             apmd                atd                 auditd              autofs
avahi-daemon        avahi-dnsconfd      bluetooth           capi                conman              corosync            cpuspeed
crond               cups                cups-config-daemon  dnsmasq             dund                firstboot           functions
gpm                 haldaemon           halt                heartbeat           hidd                hplip               httpd
ip6tables           ipmi                iptables            irda                irqbalance          iscsi               iscsid
isdn                kdump               killall             krb524              kudzu               lm_sensors          logd
lvm2-monitor        mcstrans            mdmonitor           mdmpd               messagebus          microcode_ctl       multipathd
mysqld              netconsole          netfs               netplugd            network             nfs                 nfslock
nscd                ntpd                openhpid            openibd             pacemaker           pand                pcscd
portmap             postgresql-9.0      psacct              rawdevices          rdisc               readahead_early     readahead_later
restorecond         rhnsd               rhsmcertd           rpcgssd             rpcidmapd           rpcsvcgssd          saslauthd
sendmail            setroubleshoot      single              smartd              snmpd               snmptrapd           sshd
syslog              sysstat             tomcat              vboxadd             vboxadd-service     vboxadd-x11         vncserver
vsftpd              wdaemon             winbind             wpa_supplicant      xfs                 xinetd              ypbind
yum-updatesd

[root@master ~]# crm ra list ocf heartbeat
AoEtarget           AudibleAlarm        CTDB                ClusterMon          Delay               Dummy               EvmsSCC
Evmsd               Filesystem          ICP                 IPaddr              IPaddr2             IPsrcaddr           IPv6addr
LVM                 LinuxSCSI           MailTo              ManageRAID          ManageVE            Pure-FTPd           Raid1
Route               SAPDatabase         SAPInstance         SendArp             ServeRAID           SphinxSearchDaemon  Squid
Stateful            SysInfo             VIPArip             VirtualDomain       WAS                 WAS6                WinPopup
Xen                 Xinetd              anything            apache              conntrackd          db2                 drbd
eDir88              exportfs            fio                 iSCSILogicalUnit    iSCSITarget         ids                 iscsi
jboss               ldirectord          mysql               mysql-proxy         nfsserver           nginx               oracle
oralsnr             pgsql               pingd               portblock           postfix             proftpd             rsyncd
scsi2reservation    sfex                syslog-ng           tomcat              vmware 

[root@master ~]# crm ra list ocf pacemaker
ClusterMon    Dummy         HealthCPU     HealthSMART   Stateful      SysInfo       SystemHealth  controld      o2cb          ping
pingd      

[root@master ~]# crm ra list stonith
apcmaster               apcmastersnmp           apcsmart                baytech                 bladehpi                cyclades
external/drac5          external/dracmc-telnet  external/hmchttp        external/ibmrsa         external/ibmrsa-telnet  external/ipmi
external/ippower9258    external/kdumpcheck     external/rackpdu        external/riloe          external/sbd            external/vmware
external/xen0           external/xen0-ha        fence_legacy            ibmhmc                  ipmilan                 meatware
nw_rpc100s              rcd_serial              rps10                   suicide                 wti_mpc                 wti_nps
\end{verbatim}

\begin{verbatim}
# crm ra info [class:[provider:]]resource_agent
例如：
[root@master ~]# crm ra info ocf:heartbeat:IPaddr
Manages virtual IPv4 addresses (portable version) (ocf:heartbeat:IPaddr)

This script manages IP alias IP addresses
It can add an IP alias, or remove one.

Parameters (* denotes required, [] the default):

ip* (string): IPv4 address
    The IPv4 address to be configured in dotted quad notation, for example
    "192.168.1.1".

nic (string, [eth0]): Network interface
    The base network interface on which the IP address will be brought
    online.
    
    If left empty, the script will try and determine this from the
    routing table.
    
    Do NOT specify an alias interface in the form eth0:1 or anything here;
    rather, specify the base interface only.
    
    Prerequisite:
    
    There must be at least one static IP address, which is not managed by
    the cluster, assigned to the network interface.
    
    If you can not assign any static IP address on the interface,
    modify this kernel parameter:
    sysctl -w net.ipv4.conf.all.promote_secondaries=1
    (or per device)

cidr_netmask (string): Netmask
    The netmask for the interface in CIDR format. (ie, 24), or in
    dotted quad notation  255.255.255.0).
    
    If unspecified, the script will also try to determine this from the
    routing table.

broadcast (string): Broadcast address
    Broadcast address associated with the IP. If left empty, the script will
    determine this from the netmask.

iflabel (string): Interface label
    You can specify an additional label for your IP address here.

lvs_support (boolean, [false]): Enable support for LVS DR
    Enable support for LVS Direct Routing configurations. In case a IP
    address is stopped, only move it to the loopback device to allow the
    local node to continue to service requests, but no longer advertise it
    on the network.

local_stop_script (string): 
    Script called when the IP is released

local_start_script (string): 
    Script called when the IP is added

ARP_INTERVAL_MS (integer, [500]): milliseconds between gratuitous ARPs
    milliseconds between ARPs

ARP_REPEAT (integer, [10]): repeat count
    How many gratuitous ARPs to send out when bringing up a new address

ARP_BACKGROUND (boolean, [yes]): run in background
    run in background (no longer any reason to do this)

ARP_NETMASK (string, [ffffffffffff]): netmask for ARP
    netmask for ARP - in nonstandard hexadecimal format.

Operations' defaults (advisory minimum):

    start         timeout=20s
    stop          timeout=20s
    monitor       interval=5s timeout=20s
\end{verbatim}

\section{为Web创建IP资源}

接下来要创建的web集群创建一个IP地址资源，以在通过集群提供web服务时使用；
这可以通过如下方式实现：

语法：

\begin{verbatim}
primitive <rsc> [<class>:[<provider>:]]<type>
          [params attr_list]
          [operations id_spec]
            [op op_type [<attribute>=<value>...] ...]

op_type :: start | stop | monitor
\end{verbatim}

例子：

\begin{verbatim}
 primitive apcfence stonith:apcsmart \
          params ttydev=/dev/ttyS0 hostlist="node1 node2" \
          op start timeout=60s \
          op monitor interval=30m timeout=60s
\end{verbatim}


\section{高可用验证}

\chapter{DRBD}
\label{chap:drbd}

distributed replicated block device

a software-based, shared-nothing, replicated storage solution
mirroring the content of block devices (hard disks, partitions,
logical volumes etc.) between servers.

DRBD's core functionality is implemented by way of a Linux Kernel
module.

User space administration tools

drbdadm

The high-level administration tool of the DRBD program suite. It
obtains all DRBD configuration parameters from the configuration 
file /etc/drbd.conf

drbdsetup

The program that allows users to configure the DRBD module that has
been loaded into the running kernel. It is the low-level tool within
the DRBD program suite.

Resource roles

In DRBD, every resource has a role, which may be Primary or Secondary.

Primary

A DRBD device in the primary role can be used unrestrictedly for read
and write operations. It may be used for creating and mounting file
systems, raw or direct I/O to the block device, etc.

Secondary

A DRBD device in the secondary role receives all updates from the perr
node's device, but otherwise disallows access completely.  It can not
be used by applications, neither for read nor write access.

DRBD modes

Single-primary mode
\begin{quote}
Any resource is, at any given time, in the primary role on only one
cluster member.

Only one cluster node manipulates the data at any moment.

This mode can be used with any conventional file system (ext3, ext4,
XFS, etc.)

Deploying DRBD in single-primary mode is the canonical approach for
high availability (fail-over capable) clusters.
\end{quote}

Dual-primary mode
\begin{quote}
Any resource is, at any given time, in the primary role on both
cluster nodes.

Since concurrent access to the data is thus possible, this mode
requires the use of a shared cluster file system that utilizes a
distributed lock manager. Examples include GFS and OCFS2.

Deploying DRBD in dual-primary mode is the preferred approach for
load-balancing clusters which require concurrent data access from two
nodes.

Disabled by default, and must be enabled explicitly in DRBD's
configuration file.

Available in DRBD 8.0 and later.
\end{quote}
 
\section{三种同步模式}

\begin{enumerate}[itemsep=0pt,parsep=0pt]
\item 协议A
\begin{quote}
异步复制协议。只要主节点完成本地写操作就认为写操作完成，并且需要复制的
数据包会被存储到本地TCP发送缓冲中。当发生failover故障，数据可能会丢失。
当发生failover故障时，在备节点的数据被认为仍是稳固的，然而，在crash发生
的时间点上很多最新的更新操作会丢失。
\end{quote}

\item 协议B
\begin{quote}
内存同步（半同步）复制协议。当本地盘的写已经完成，并且复制数据包已经到
达对应从节点，此时主节点才认为磁盘写已经完成。通常情况下，发生failover
不会导致数据丢失（因为后备系统内存中已经获得了数据更新）。然而，如果所
有节点同时出现电源故障，则主节点数据存储会产生不可逆的错误结构，主节点
上多数最新写入的数据可能会丢失。
\end{quote}

\item 协议C
\begin{quote}
同步复制协议。只有在本地和远程磁盘都确定写入已经完成时，主节点才会认为
写入完成。这样可确保发生故障时不会导致任何数据丢失。如果发生数据丢失的
现象，那也只会在所有节点同时存在错误存储时才会发生。
\end{quote}
\end{enumerate}

Efficient synchronization
\begin{quote}
Synchronization is necessary if the replication link has been
interrupted for any reason

1. failure of the primary node
2. failure of the secondary node
3. or interruption of the replication link

(Re-)synchronization is distinct from device replication
\end{quote}

Split brain
\begin{quote}
Split brain is a situation where, due to temporary failure of all
network links between cluster nodes, and possibly due to intervension
by a cluster management software or human error, both nodes switched
to the primary role while disconnected.
\end{quote}

automatic recovery
\begin{quote}
DRBD allows for automatic operator notification (by email or other
means) when it detects split brain.

DRBD has several resolution algorithms available for resolving the
split brain automaticly.

1. Discarding modifications made on the "younger" primary
2. Discarding modifications made on the "older" primary
3. Discarding modifications on the primary with fewer changes
4. Graceful recovery from split brain if one host has had no intermediate changes
\end{quote}

\section{安装及配置DRBD}

DRBD包含两大主要组件，内核空间中的驱动代码与用户空间中的管理工具。自从
Linux内核2.6.33版本及之后版本，DRBD的驱动代码已整合进了内核模块。所以，
在2.6.33版本及更新版本后，我们只需要安装用户空间的管理工具即可。否则，
我们需要同时安装内核支持模块与管理工具两个包，并且两个工具的版本号要保
持一致。

实验环境：

\begin{figure}[!h]
\centering
\begin{tabular}{llll}
\toprule
机器列表        & IP地址        & 操作系统       & DRBD版本 \\
\midrule
node1.laven.com & 192.168.1.15  & RHEL5U5 32bit  & 8.3.8 \\
node2.laven.com & 192.168.1.16  & RHEL5U5 32bit  & 8.3.8 \\
\bottomrule
\end{tabular}
\end{figure}


开始安装

\small{
\begin{verbatim}
[root@node1 ~]# ls
drbd83-8.3.8-1.el5.centos.i386.rpm
kmod-drbd83-8.3.8-1.el5.centos.i386.rpm

[root@node1 ~]# rpm -ivh kmod-drbd83-8.3.8-1.el5.centos.i386.rpm
[root@node1 ~]# rpm -ivh drbd83-8.3.8-1.el5.centos.i386.rpm
\end{verbatim}
}
\normalsize

在node2上进行同样的安装操作。

开始配置，

\small{
\begin{verbatim}
[root@node1 ~]# ls
drbd83-8.3.8-1.el5.centos.i386.rpm
kmod-drbd83-8.3.8-1.el5.centos.i386.rpm

[root@node1 ~]# rpm -ivh kmod-drbd83-8.3.8-1.el5.centos.i386.rpm
[root@node1 ~]# rpm -ivh drbd83-8.3.8-1.el5.centos.i386.rpm
\end{verbatim}
}
\normalsize

安装完成之后，来个简单的配置，使drbd可以简单的用起来。drbd的主配置文件
为/etc/drbd.conf，其中还有一个/etc/drbd.d目录，里面放置了其他的配置文件，
这也是便于管理，因此，我们可以分段进行配置，然后放置到该目录即可，在主
配置文件中使用include指令把这些文件加载进来即可。

看一下全局配置文件，

\small{
\begin{verbatim}
[root@node1 ~]# cat /etc/drbd.d/global_common.conf 
global {
   usage-count no;
   # minor-count dialog-refresh disable-ip-verification
}

common {
   protocol C;

   handlers {
      pri-on-incon-degr "/usr/lib/drbd/notify-pri-on-incon-degr.sh; \
      /usr/lib/drbd/notify-emergency-reboot.sh; \
      echo b > /proc/sysrq-trigger ; reboot -f";

      pri-lost-after-sb "/usr/lib/drbd/notify-pri-lost-after-sb.sh; \
      /usr/lib/drbd/notify-emergency-reboot.sh; \
      echo b > /proc/sysrq-trigger ; reboot -f";

      local-io-error "/usr/lib/drbd/notify-io-error.sh; \
      /usr/lib/drbd/notify-emergency-shutdown.sh; \
      echo o > /proc/sysrq-trigger ; halt -f";

      fence-peer "/usr/lib/drbd/crm-fence-peer.sh";
      split-brain "/usr/lib/drbd/notify-split-brain.sh root";
      out-of-sync "/usr/lib/drbd/notify-out-of-sync.sh root";
      before-resync-target "/usr/lib/drbd/snapshot-resync-target-lvm.sh \
      -p 15 -- -c 16k";
       after-resync-target /usr/lib/drbd/unsnapshot-resync-target-lvm.sh;
   }

   startup {
       wfc-timeout 120;
       degr-wfc-timeout 120;
   }

   disk {
       on-io-error detach;
       fencing resource-only;
   }

   net {
       cram-hmac-alg "sha1";
       shared-secret "mypass";
   }

   syncer {
       rate 100M;
   }
}
\end{verbatim}
}
\normalsize

现在定一个简单的资源r0,资源文件后缀以res结尾。

\small{
\begin{verbatim}
[root@node1 ~]# cat /etc/drbd.d/r0.res 
resource r0 {
  on node1.laven.com {
    device    /dev/drbd0;
    disk      /dev/sdb1;
    address   192.168.1.15:7789;
    meta-disk internal;
  }

  on node2.laven.com {
    device    /dev/drbd0;
    disk      /dev/sdb1;
    address   192.168.1.16:7789;
    meta-disk internal;
  }
}
\end{verbatim}
}
\normalsize

配置完毕，我们可以把这些文件复制到节点node2上去，

\small{
\begin{verbatim}
[root@node1 ~]# scp /etc/drbd.d/* node2:/etc/drbd.d/
\end{verbatim}
}
\normalsize

好了，初始化两端的drbd，然后启动drbd服务。

\small{
\begin{verbatim}
[root@node1 ~]# drbdadm create-md r0
[root@node2 ~]# drbdadm create-md r0
\end{verbatim}
}
\normalsize

初始化完毕之后，我们把node1作为主节点，node2作为从节点。之后，我们把
/dev/drbd0格式化然后挂载到/web目录下，然后在/web路下创建一些文件，然后
到node2上检查这些文件是否可以看到。

\small{
\begin{verbatim}
[root@node1 ~]# fdisk /dev/sdb -> /dev/sdb1
[root@node1 ~]# mke2fs -j /dev/drbd0
[root@node1 ~]# mount /dev/drbd0 /web
[root@node1 ~]# drbdadm primary r0
[root@node1 ~]# cd /web
[root@node1 web]# for i in {1..10}; do echo "Hello, world" > file${i}; \
> done
[root@node1 web]# ls
file1  file10  file2  file3  file4  file5  
file6  file7   file8  file9  lost+found
\end{verbatim}
}
\normalsize

这时，我们把node2作为主节点，并把/dev/drbd0挂载到/web目录下，验证是不是
有上述文件。首先，需要把/web目录umount，然后把node1变为从节点。之后，
node2需要先把自己变为主节点，然后挂载/dev/drbd0到/web下即可，

\small{
\begin{verbatim}
[root@node1 ~]# umount /web
[root@node1 ~]# drbdadm secondary r0
[root@node2 ~]# drbdadm primary r0
[root@node2 ~]# mount /dev/drbd0 /web
[root@node2 ~]# ls /web
file1  file10  file2  file3  file4  file5  
file6  file7   file8  file9  lost+found
[root@node2 ~]# cat /web/file*
Hello, world
Hello, world
Hello, world
Hello, world
Hello, world
Hello, world
Hello, world
Hello, world
Hello, world
Hello, world
\end{verbatim}
}
\normalsize

\input{body/chapter11_haproxy}
